{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEj5rJ13wU8K",
        "outputId": "37523faa-d0a6-4d37-b815-4b89ee1ac07e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nytyrf_8wPvQ",
        "outputId": "6fd28007-c82c-416f-9ffb-c815fd168eef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7648b120-630c-476c-82be-7499ca6bf3ff\", \"train.json\", 711336801)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9385464d-16da-42f9-b917-d01d163092c1\", \"dev.json\", 152470696)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ff39f118-d73b-4fb1-ae3f-4832876ee92f\", \"test.json\", 152433208)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed. Files are ready for download.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 仅在 Colab 环境下使用，下载生成的文件\n",
        "from google.colab import files\n",
        "\n",
        "def merge_ingredient_with_ner(ingredient: str, ner: list) -> str:\n",
        "    \"\"\"\n",
        "    如果 NER 列表中存在与 ingredient 部分匹配的短语，\n",
        "    则返回该完整的 NER 短语；否则返回 ingredient 本身。\n",
        "    例如：ingredient 为 \"chicken\" 而 NER 中有 \"chicken tender\"，则返回 \"chicken tender\"。\n",
        "    \"\"\"\n",
        "    ingredient_tokens = set(ingredient.lower().split())\n",
        "    for ner_item in ner:\n",
        "        ner_tokens = set(ner_item.lower().split())\n",
        "        # 如果 ingredient 中的单词集合与 NER 短语有交集，则认为匹配成功\n",
        "        if ingredient_tokens.issubset(ner_tokens) or ingredient_tokens.intersection(ner_tokens):\n",
        "            return ner_item  # 使用 NER 中识别到的更完整短语\n",
        "    return ingredient\n",
        "\n",
        "def get_ingredient_entry(ingredient: str, title: str, ner: list) -> dict:\n",
        "    \"\"\"\n",
        "    根据标题和 NER 中的食物短语判断食材是否为核心食材，\n",
        "    如果标题中包含该食材相关词汇，则返回合并后的食材名称，并添加字段 type: \"core\"；\n",
        "    否则仅返回食材名称。\n",
        "    \"\"\"\n",
        "    merged = merge_ingredient_with_ner(ingredient, ner)\n",
        "    title_words = set(title.lower().split())\n",
        "    ing_tokens = set(merged.lower().split())\n",
        "    # 如果合并后的食材名称中有单词出现在标题中，则认为该食材为核心食材\n",
        "    if ing_tokens.intersection(title_words):\n",
        "        return {'name': merged, 'type': 'core'}\n",
        "    else:\n",
        "        return {'name': ingredient}\n",
        "\n",
        "def get_dietary_tags(ingredients: list) -> list:\n",
        "    \"\"\"基本饮食标签——仅检查素食/无麸质\"\"\"\n",
        "    tags = []\n",
        "    ingredients_str = ' '.join(ingredients).lower()\n",
        "\n",
        "    # 素食判断\n",
        "    if not any(meat in ingredients_str for meat in ['chicken', 'beef', 'pork', 'fish']):\n",
        "        tags.append('vegetarian')\n",
        "\n",
        "    # 无麸质判断\n",
        "    if not any(gluten in ingredients_str for gluten in ['flour', 'wheat', 'bread']):\n",
        "        tags.append('gluten-free')\n",
        "\n",
        "    return tags\n",
        "\n",
        "def get_course_type(title: str) -> str:\n",
        "    \"\"\"基础菜品类型判断\"\"\"\n",
        "    title_lower = str(title).lower()\n",
        "    if any(keyword in title_lower for keyword in ['dessert', 'cake', 'pie']):\n",
        "        return 'dessert'\n",
        "    if any(keyword in title_lower for keyword in ['salad', 'soup', 'side']):\n",
        "        return 'side'\n",
        "    return 'main'\n",
        "\n",
        "def safe_eval(data, default):\n",
        "    \"\"\"安全地解析字符串表示的列表\"\"\"\n",
        "    if isinstance(data, str):\n",
        "        try:\n",
        "            return eval(data)\n",
        "        except:\n",
        "            return default\n",
        "    return data if data is not None else default\n",
        "\n",
        "def clean_recipe(recipe: dict) -> dict:\n",
        "    \"\"\"数据清洗函数，并进行基础的分类处理\"\"\"\n",
        "    try:\n",
        "        # 检查来源（假设 source 为 \"Recipes1M\" 或 \"Gathered\"）\n",
        "        if str(recipe.get('source')) != \"Recipes1M\":\n",
        "            return None\n",
        "\n",
        "        title = str(recipe['title']).strip() if pd.notna(recipe['title']) else \"\"\n",
        "        ingredients = [str(x).strip() for x in safe_eval(recipe['ingredients'], []) if x and str(x).strip()]\n",
        "        # 处理 NER 字段，获取食物短语列表\n",
        "        ner_list = [str(x).strip() for x in safe_eval(recipe['NER'], []) if x and str(x).strip()]\n",
        "\n",
        "        return {\n",
        "            'id': int(recipe['id']),\n",
        "            'title': title,\n",
        "            'ingredients': [get_ingredient_entry(ing, title, ner_list) for ing in ingredients],\n",
        "            'directions': [str(x).strip() for x in safe_eval(recipe['directions'], []) if x and str(x).strip()],\n",
        "            'dietary': get_dietary_tags(ingredients),\n",
        "            'course_type': get_course_type(title),\n",
        "            'ner': ner_list\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping recipe {recipe.get('id')}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_dataset(input_csv: str, output_files: dict):\n",
        "    \"\"\"数据集处理主流程，将数据写入当前工作目录下的文件并触发下载\"\"\"\n",
        "    # 读取 CSV 数据\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # 清洗及过滤数据\n",
        "    cleaned_data = [x for x in map(clean_recipe, df.to_dict('records')) if x is not None]\n",
        "\n",
        "    # 划分数据集\n",
        "    train, temp = train_test_split(cleaned_data, test_size=0.3, random_state=42)\n",
        "    dev, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # 分别写入文件并下载\n",
        "    splits = {'train': train, 'dev': dev, 'test': test}\n",
        "    for split, data in splits.items():\n",
        "        file_name = output_files[split]\n",
        "        with open(file_name, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        # 下载文件到本地\n",
        "        files.download(file_name)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        'input_csv': '/content/drive/MyDrive/Spring 2025/CS469/RecipeNLG_dataset.csv',  # CSV 文件的路径，请确保该路径正确\n",
        "        'output_files': {\n",
        "            # 改为保存到当前工作目录，不再使用固定盘符路径\n",
        "            'train': 'train.json',\n",
        "            'dev': 'dev.json',\n",
        "            'test': 'test.json'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    process_dataset(config['input_csv'], config['output_files'])\n",
        "    print(\"Processing completed. Files are ready for download.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text: str) -> list:\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "def compute_tf(tokens: list) -> dict:\n",
        "    \"\"\"\n",
        "    Compute the term frequency (TF) for a list of tokens.\n",
        "    \"\"\"\n",
        "    tf_counter = Counter(tokens)\n",
        "    total_tokens = len(tokens)\n",
        "    tf = {term: count / total_tokens for term, count in tf_counter.items()}\n",
        "    return tf\n",
        "\n",
        "def compute_idf(documents_tokens: list) -> dict:\n",
        "    \"\"\"\n",
        "    Compute the IDF for each term in a corpus.\n",
        "    \"\"\"\n",
        "    N = len(documents_tokens)\n",
        "    idf = {}\n",
        "    for tokens in documents_tokens:\n",
        "        unique_tokens = set(tokens)\n",
        "        for term in unique_tokens:\n",
        "            idf[term] = idf.get(term, 0) + 1\n",
        "\n",
        "    # Convert document frequency to idf score using logarithm\n",
        "    for term, df in idf.items():\n",
        "        idf[term] = math.log(N / df)\n",
        "    return idf\n",
        "\n",
        "def compute_tfidf(documents: list) -> list:\n",
        "    \"\"\"\n",
        "    Compute the TF-IDF for each document in a corpus.\n",
        "    \"\"\"\n",
        "    # Tokenize each document\n",
        "    tokenized_docs = [tokenize(doc) for doc in documents]\n",
        "\n",
        "    # Compute IDF using all tokenized documents\n",
        "    idf = compute_idf(tokenized_docs)\n",
        "\n",
        "    tfidf_documents = []\n",
        "    for tokens in tokenized_docs:\n",
        "        tf = compute_tf(tokens)\n",
        "        tfidf = {term: tf_val * idf.get(term, 0) for term, tf_val in tf.items()}\n",
        "        tfidf_documents.append(tfidf)\n",
        "\n",
        "    return tfidf_documents"
      ],
      "metadata": {
        "id": "UhKk0VFdZTFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GzF2IF2N_JO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qz_jyyJH5fmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}